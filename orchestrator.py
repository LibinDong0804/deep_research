"""
Orchestrator - ç ”ç©¶ä»»åŠ¡åè°ƒè€… (Lead Agent)

åŸºäºé«˜çº§ç ”ç©¶æ™ºèƒ½ä½“è®¾è®¡ä¼˜åŒ–ï¼š
- æŸ¥è¯¢ç±»å‹åˆ†ç±»ï¼ˆæ·±åº¦ä¼˜å…ˆ/å¹¿åº¦ä¼˜å…ˆ/ç®€å•æŸ¥è¯¢ï¼‰
- åŠ¨æ€å­æ™ºèƒ½ä½“æ•°é‡è°ƒæ•´
- æ™ºèƒ½ç»ˆæ­¢ï¼ˆæ”¶ç›Šé€’å‡æ£€æµ‹ï¼‰
- æ‰¹åˆ¤æ€§ä¿¡æ¯éªŒè¯

è´Ÿè´£ï¼š
1. ä»»åŠ¡æ‹†è§£ï¼šå°†å¤æ‚ç ”ç©¶ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡
2. æŸ¥è¯¢ç±»å‹åˆ¤æ–­ï¼šç¡®å®šæœ€ä¼˜ç ”ç©¶ç­–ç•¥
3. åŠ¨æ€è°ƒåº¦ï¼šæ ¹æ®å¤æ‚åº¦è°ƒæ•´å­æ™ºèƒ½ä½“æ•°é‡
4. ä»»åŠ¡åˆ†å‘ï¼šå°†å­ä»»åŠ¡åˆ†é…ç»™ Workers
5. ç»“æœæ±‡æ€»ï¼šæ”¶é›†å¹¶æ•´åˆ Worker ç»“æœ
6. è´¨é‡æ§åˆ¶ï¼šè¿­ä»£ä¼˜åŒ–ç›´è‡³è¾¾æ ‡æˆ–æ”¶ç›Šé€’å‡
7. æŠ¥å‘Šç”Ÿæˆï¼šç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š
"""

import json
import time
from dataclasses import dataclass, field
from typing import Optional, Callable, Literal
from concurrent.futures import ThreadPoolExecutor, as_completed

from .tools import get_llm_client, extract_json, LLMResponse
from .prompts import (
    ORCHESTRATOR_SYSTEM_PROMPT,
    TASK_DECOMPOSITION_PROMPT,
    SYNTHESIS_PROMPT,
    QUALITY_CHECK_PROMPT,
    GAP_ANALYSIS_PROMPT,
    REPORT_REFINEMENT_PROMPT,
    DIMINISHING_RETURNS_CHECK_PROMPT,
    PARALLEL_TASK_COORDINATION_PROMPT,
)
from .config import AGENT_CONFIG


# ==================== æ•°æ®ç»“æ„ ====================

@dataclass
class SubTask:
    """å­ä»»åŠ¡æ•°æ®ç»“æ„ - å¢å¼ºç‰ˆ"""
    id: str
    name: str
    description: str
    search_queries: list[str]
    priority: str = "medium"
    expected_output: str = ""
    status: str = "pending"  # pending, running, completed, failed
    result: Optional[dict] = None
    # æ–°å¢å­—æ®µ
    research_objective: str = ""
    expected_sources: list[str] = field(default_factory=list)
    scope_boundaries: str = ""


@dataclass
class QueryTypeAnalysis:
    """æŸ¥è¯¢ç±»å‹åˆ†æç»“æœ"""
    query_type: Literal["depth_first", "breadth_first", "simple"]
    reasoning: str
    recommended_approach: str
    recommended_worker_count: int


@dataclass
class TaskPlan:
    """ä»»åŠ¡è®¡åˆ’æ•°æ®ç»“æ„ - å¢å¼ºç‰ˆ"""
    task_understanding: dict
    subtasks: list[SubTask]
    report_structure: dict
    # æ–°å¢å­—æ®µ
    query_type: QueryTypeAnalysis = None
    research_plan: dict = field(default_factory=dict)
    worker_count: dict = field(default_factory=dict)


@dataclass
class IterationRecord:
    """å•æ¬¡è¿­ä»£è®°å½•"""
    iteration: int
    report: str
    quality_score: int
    quality_result: dict
    gap_analysis: Optional[dict] = None
    supplementary_results: list[dict] = field(default_factory=list)
    # æ–°å¢å­—æ®µ
    diminishing_returns_detected: bool = False
    score_improvement: int = 0


@dataclass
class ResearchState:
    """ç ”ç©¶çŠ¶æ€æ•°æ®ç»“æ„"""
    original_task: str
    task_plan: Optional[TaskPlan] = None
    worker_results: list[dict] = field(default_factory=list)
    final_report: str = ""
    quality_score: int = 0
    status: str = "initialized"  # initialized, planning, researching, synthesizing, iterating, completed, failed
    error_msg: str = ""
    start_time: float = 0
    end_time: float = 0
    # è¿­ä»£ä¼˜åŒ–ç›¸å…³
    iteration_count: int = 0
    iteration_history: list[IterationRecord] = field(default_factory=list)
    # æ–°å¢å­—æ®µ
    query_type: str = ""
    early_termination_reason: str = ""


# ==================== Orchestrator ç±» ====================

class Orchestrator:
    """
    ç ”ç©¶ä»»åŠ¡åè°ƒè€… (Lead Agent)

    å¢å¼ºåŠŸèƒ½ï¼š
    - æŸ¥è¯¢ç±»å‹åˆ†ç±»å’Œç­–ç•¥åˆ¶å®š
    - åŠ¨æ€å­æ™ºèƒ½ä½“æ•°é‡è°ƒæ•´
    - æ™ºèƒ½ç»ˆæ­¢ï¼ˆæ”¶ç›Šé€’å‡æ£€æµ‹ï¼‰
    - æ‰¹åˆ¤æ€§ä¿¡æ¯éªŒè¯
    """

    def __init__(
        self,
        worker_factory: Callable,
        max_workers: int = None,
        on_progress: Callable[[str, dict], None] = None,
    ):
        """
        åˆå§‹åŒ– Orchestrator

        å‚æ•°:
            worker_factory: Worker å·¥å‚å‡½æ•°ï¼Œç”¨äºåˆ›å»º Worker å®ä¾‹
            max_workers: æœ€å¤§å¹¶è¡Œ Worker æ•°
            on_progress: è¿›åº¦å›è°ƒå‡½æ•° (stage, data)
        """
        self.llm = get_llm_client()
        self.worker_factory = worker_factory
        self.max_workers = max_workers or AGENT_CONFIG["max_workers"]
        self.on_progress = on_progress or (lambda stage, data: None)

    def _emit_progress(self, stage: str, data: dict = None):
        """å‘é€è¿›åº¦äº‹ä»¶"""
        self.on_progress(stage, data or {})

    def decompose_task(self, task: str) -> TaskPlan:
        """
        å°†ç ”ç©¶ä»»åŠ¡æ‹†è§£ä¸ºå­ä»»åŠ¡ï¼ˆå¢å¼ºç‰ˆï¼‰

        æ–°åŠŸèƒ½ï¼š
        - æŸ¥è¯¢ç±»å‹åˆ¤æ–­
        - åŠ¨æ€å­æ™ºèƒ½ä½“æ•°é‡å»ºè®®
        - è¯¦ç»†ç ”ç©¶è®¡åˆ’

        å‚æ•°:
            task: åŸå§‹ç ”ç©¶ä»»åŠ¡æè¿°

        è¿”å›:
            TaskPlan: ä»»åŠ¡è®¡åˆ’
        """
        self._emit_progress("decomposing", {"task": task})

        prompt = TASK_DECOMPOSITION_PROMPT.format(task=task)

        response = self.llm.chat(
            prompt=prompt,
            system_prompt=ORCHESTRATOR_SYSTEM_PROMPT,
            temperature=0.3,  # ä»»åŠ¡æ‹†è§£éœ€è¦ç¨³å®šæ€§
        )

        if not response.success:
            raise Exception(f"ä»»åŠ¡æ‹†è§£å¤±è´¥: {response.error_msg}")

        # è§£æ JSON ç»“æœ
        result = extract_json(response.content)

        if not result:
            raise Exception("æ— æ³•è§£æä»»åŠ¡æ‹†è§£ç»“æœ")

        # è§£ææŸ¥è¯¢ç±»å‹
        query_type_data = result.get("query_type", {})
        worker_count_data = result.get("worker_count", {})

        query_type = QueryTypeAnalysis(
            query_type=query_type_data.get("type", "breadth_first"),
            reasoning=query_type_data.get("reasoning", ""),
            recommended_approach=query_type_data.get("recommended_approach", ""),
            recommended_worker_count=worker_count_data.get("recommended", 3),
        )

        # æ„å»ºå­ä»»åŠ¡åˆ—è¡¨ï¼ˆå¢å¼ºç‰ˆï¼‰
        subtasks = []
        for st in result.get("subtasks", []):
            subtasks.append(SubTask(
                id=st.get("id", f"task_{len(subtasks)+1}"),
                name=st.get("name", "æœªå‘½åä»»åŠ¡"),
                description=st.get("description", ""),
                search_queries=st.get("search_queries", []),
                priority=st.get("priority", "medium"),
                expected_output=st.get("expected_output", ""),
                # æ–°å¢å­—æ®µ
                research_objective=st.get("research_objective", ""),
                expected_sources=st.get("expected_sources", []),
                scope_boundaries=st.get("scope_boundaries", ""),
            ))

        # æ ¹æ®æŸ¥è¯¢ç±»å‹å’Œå»ºè®®è°ƒæ•´å­ä»»åŠ¡æ•°é‡
        recommended_count = query_type.recommended_worker_count
        if len(subtasks) > recommended_count:
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œä¿ç•™æœ€é‡è¦çš„ä»»åŠ¡
            priority_order = {"high": 0, "medium": 1, "low": 2}
            subtasks.sort(key=lambda x: priority_order.get(x.priority, 1))
            subtasks = subtasks[:recommended_count]

        task_plan = TaskPlan(
            task_understanding=result.get("task_understanding", {}),
            subtasks=subtasks,
            report_structure=result.get("report_structure", {}),
            query_type=query_type,
            research_plan=result.get("research_plan", {}),
            worker_count=worker_count_data,
        )

        self._emit_progress("decomposed", {
            "subtask_count": len(subtasks),
            "query_type": query_type.query_type,
            "recommended_workers": query_type.recommended_worker_count,
            "subtasks": [{"id": st.id, "name": st.name, "priority": st.priority} for st in subtasks],
        })

        return task_plan

    def dispatch_workers(self, task_plan: TaskPlan) -> list[dict]:
        """
        åˆ†å‘å­ä»»åŠ¡ç»™ Workers å¹¶æ”¶é›†ç»“æœ

        æ ¹æ®æŸ¥è¯¢ç±»å‹ä¼˜åŒ–è°ƒåº¦ç­–ç•¥ï¼š
        - æ·±åº¦ä¼˜å…ˆï¼šæŒ‰è§†è§’åˆ†é…
        - å¹¿åº¦ä¼˜å…ˆï¼šæŒ‰å­ä¸»é¢˜åˆ†é…
        - ç®€å•æŸ¥è¯¢ï¼šå•ä¸ª Worker

        å‚æ•°:
            task_plan: ä»»åŠ¡è®¡åˆ’

        è¿”å›:
            list[dict]: Worker ç»“æœåˆ—è¡¨
        """
        query_type = task_plan.query_type.query_type if task_plan.query_type else "breadth_first"

        self._emit_progress("dispatching", {
            "worker_count": len(task_plan.subtasks),
            "query_type": query_type,
        })

        results = []

        # ç¡®å®šå¹¶è¡Œæ•°é‡
        actual_workers = min(
            len(task_plan.subtasks),
            self.max_workers,
            task_plan.query_type.recommended_worker_count if task_plan.query_type else self.max_workers
        )

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=actual_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {}
            for subtask in task_plan.subtasks:
                worker = self.worker_factory()
                future = executor.submit(worker.execute, subtask)
                future_to_task[future] = subtask

                self._emit_progress("worker_started", {
                    "task_id": subtask.id,
                    "task_name": subtask.name,
                    "priority": subtask.priority,
                })

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_task):
                subtask = future_to_task[future]
                try:
                    result = future.result(timeout=AGENT_CONFIG["timeout_per_worker"])
                    subtask.status = "completed"
                    subtask.result = result
                    results.append(result)

                    self._emit_progress("worker_completed", {
                        "task_id": subtask.id,
                        "task_name": subtask.name,
                        "success": True,
                        "confidence_level": result.get("confidence_level", "unknown"),
                    })

                except Exception as e:
                    subtask.status = "failed"
                    results.append({
                        "task_id": subtask.id,
                        "task_name": subtask.name,
                        "error": str(e),
                        "success": False,
                    })

                    self._emit_progress("worker_failed", {
                        "task_id": subtask.id,
                        "task_name": subtask.name,
                        "error": str(e),
                    })

        return results

    def synthesize_results(
        self,
        original_task: str,
        task_plan: TaskPlan,
        worker_results: list[dict],
    ) -> str:
        """
        æ±‡æ€» Worker ç»“æœç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰

        æ–°åŠŸèƒ½ï¼š
        - åŸºäºæŸ¥è¯¢ç±»å‹çš„ç»¼åˆç­–ç•¥
        - æ¥æºè´¨é‡ä¿¡æ¯æ•´åˆ
        - ä¿¡æ¯å†²çªè¯†åˆ«

        å‚æ•°:
            original_task: åŸå§‹ç ”ç©¶ä»»åŠ¡
            task_plan: ä»»åŠ¡è®¡åˆ’
            worker_results: Worker ç»“æœåˆ—è¡¨

        è¿”å›:
            str: æœ€ç»ˆç ”ç©¶æŠ¥å‘Š
        """
        self._emit_progress("synthesizing", {
            "result_count": len(worker_results),
        })

        # æ ¼å¼åŒ– Worker ç»“æœ
        results_text = self._format_worker_results(worker_results)

        # æ ¼å¼åŒ–æŠ¥å‘Šç»“æ„
        report_structure = json.dumps(
            task_plan.report_structure,
            ensure_ascii=False,
            indent=2,
        )

        # æ ¼å¼åŒ–æŸ¥è¯¢ç±»å‹åˆ†æ
        query_type_analysis = ""
        if task_plan.query_type:
            query_type_analysis = f"""
æŸ¥è¯¢ç±»å‹: {task_plan.query_type.query_type}
åˆ¤æ–­ä¾æ®: {task_plan.query_type.reasoning}
ç ”ç©¶æ–¹æ³•: {task_plan.query_type.recommended_approach}
"""

        prompt = SYNTHESIS_PROMPT.format(
            original_task=original_task,
            query_type_analysis=query_type_analysis,
            report_structure=report_structure,
            worker_results=results_text,
        )

        response = self.llm.chat(
            prompt=prompt,
            system_prompt=ORCHESTRATOR_SYSTEM_PROMPT,
            temperature=0.5,
            max_tokens=8192,
        )

        if not response.success:
            raise Exception(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {response.error_msg}")

        self._emit_progress("synthesized", {
            "report_length": len(response.content),
        })

        return response.content

    def check_quality(self, report: str, original_task: str, query_type: str = "") -> dict:
        """
        å¯¹æŠ¥å‘Šè¿›è¡Œè´¨é‡æ£€æŸ¥ï¼ˆå¢å¼ºç‰ˆï¼‰

        æ–°åŠŸèƒ½ï¼š
        - åŸºäºæŸ¥è¯¢ç±»å‹çš„è¯„ä¼°æ ‡å‡†
        - æ¥æºè´¨é‡è¯„ä¼°
        - äº‹å®å†²çªæ£€æµ‹

        å‚æ•°:
            report: ç ”ç©¶æŠ¥å‘Š
            original_task: åŸå§‹ç ”ç©¶ä»»åŠ¡
            query_type: æŸ¥è¯¢ç±»å‹

        è¿”å›:
            dict: è´¨é‡æ£€æŸ¥ç»“æœ
        """
        self._emit_progress("quality_checking", {})

        prompt = QUALITY_CHECK_PROMPT.format(
            report=report,
            original_task=original_task,
            query_type=query_type or "æœªæŒ‡å®š",
        )

        response = self.llm.chat(
            prompt=prompt,
            system_prompt=ORCHESTRATOR_SYSTEM_PROMPT,
            temperature=0.2,
        )

        if not response.success:
            return {"overall_score": 0, "error": response.error_msg}

        result = extract_json(response.content)
        if not result:
            result = {"overall_score": 70, "note": "è´¨é‡æ£€æŸ¥ç»“æœè§£æå¤±è´¥"}

        self._emit_progress("quality_checked", {
            "score": result.get("overall_score", 0),
            "needs_revision": result.get("needs_revision", False),
            "revision_priority": result.get("revision_priority", "low"),
        })

        return result

    def check_diminishing_returns(
        self,
        original_task: str,
        current_score: int,
        iteration_history: list[IterationRecord],
        recent_supplementary_results: list[dict],
    ) -> dict:
        """
        æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ”¶ç›Šé€’å‡ç‚¹

        å‚æ•°:
            original_task: åŸå§‹ä»»åŠ¡
            current_score: å½“å‰è´¨é‡åˆ†æ•°
            iteration_history: è¿­ä»£å†å²
            recent_supplementary_results: æœ€è¿‘çš„è¡¥å……ç ”ç©¶ç»“æœ

        è¿”å›:
            dict: æ”¶ç›Šé€’å‡æ£€æŸ¥ç»“æœ
        """
        # æ„å»ºè¿­ä»£å†å²æ‘˜è¦
        history_summary = []
        for record in iteration_history:
            history_summary.append({
                "iteration": record.iteration,
                "score": record.quality_score,
                "improvement": record.score_improvement,
            })

        prompt = DIMINISHING_RETURNS_CHECK_PROMPT.format(
            original_task=original_task,
            current_score=current_score,
            iteration_history=json.dumps(history_summary, ensure_ascii=False),
            recent_supplementary_results=json.dumps(
                recent_supplementary_results[:3],  # åªå–æœ€è¿‘çš„ç»“æœ
                ensure_ascii=False,
                indent=2,
            ) if recent_supplementary_results else "æ— ",
        )

        response = self.llm.chat(
            prompt=prompt,
            system_prompt=ORCHESTRATOR_SYSTEM_PROMPT,
            temperature=0.2,
        )

        if not response.success:
            return {"diminishing_returns_detected": False}

        result = extract_json(response.content)
        return result or {"diminishing_returns_detected": False}

    def analyze_gaps(
        self,
        original_task: str,
        current_report: str,
        quality_result: dict,
        iteration: int,
        max_iterations: int,
        query_type: str = "",
    ) -> dict:
        """
        åˆ†ææŠ¥å‘Šçš„å·®è·å’Œä¸è¶³ï¼ˆå¢å¼ºç‰ˆï¼‰

        å‚æ•°:
            original_task: åŸå§‹ç ”ç©¶ä»»åŠ¡
            current_report: å½“å‰æŠ¥å‘Š
            quality_result: è´¨é‡æ£€æŸ¥ç»“æœ
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            query_type: æŸ¥è¯¢ç±»å‹

        è¿”å›:
            dict: å·®è·åˆ†æç»“æœï¼ŒåŒ…å«è¡¥å……ä»»åŠ¡
        """
        self._emit_progress("analyzing_gaps", {"iteration": iteration})

        prompt = GAP_ANALYSIS_PROMPT.format(
            original_task=original_task,
            query_type=query_type or "æœªæŒ‡å®š",
            current_report=current_report,
            quality_result=json.dumps(quality_result, ensure_ascii=False, indent=2),
            iteration=iteration,
            max_iterations=max_iterations,
        )

        response = self.llm.chat(
            prompt=prompt,
            system_prompt=ORCHESTRATOR_SYSTEM_PROMPT,
            temperature=0.3,
        )

        if not response.success:
            return {"error": response.error_msg, "supplementary_tasks": []}

        result = extract_json(response.content)
        if not result:
            result = {
                "supplementary_tasks": [],
                "gap_analysis": {},
                "refinement_focus": [],
                "stop_iteration_recommendation": {"should_stop": False}
            }

        self._emit_progress("gaps_analyzed", {
            "iteration": iteration,
            "gap_count": len(result.get("gap_analysis", {}).get("completeness_gaps", {}).get("missing_aspects", [])),
            "task_count": len(result.get("supplementary_tasks", [])),
            "should_stop": result.get("stop_iteration_recommendation", {}).get("should_stop", False),
        })

        return result

    def dispatch_supplementary_workers(self, supplementary_tasks: list[dict]) -> list[dict]:
        """
        åˆ†å‘è¡¥å……ç ”ç©¶ä»»åŠ¡ç»™ Workers

        å‚æ•°:
            supplementary_tasks: è¡¥å……ä»»åŠ¡åˆ—è¡¨

        è¿”å›:
            list[dict]: è¡¥å……ç ”ç©¶ç»“æœ
        """
        if not supplementary_tasks:
            return []

        self._emit_progress("dispatching_supplementary", {
            "task_count": len(supplementary_tasks),
        })

        # å°†è¡¥å……ä»»åŠ¡è½¬æ¢ä¸º SubTask æ ¼å¼
        subtasks = []
        for st in supplementary_tasks:
            subtasks.append(SubTask(
                id=st.get("id", f"sup_task_{len(subtasks)+1}"),
                name=st.get("name", "è¡¥å……ç ”ç©¶ä»»åŠ¡"),
                description=st.get("description", ""),
                search_queries=st.get("search_queries", []),
                priority=st.get("priority", "high"),
                # æ–°å¢å­—æ®µ
                research_objective=st.get("research_objective", ""),
                expected_sources=st.get("expected_sources", []),
                scope_boundaries=st.get("scope_boundaries", ""),
            ))

        results = []

        with ThreadPoolExecutor(max_workers=min(len(subtasks), self.max_workers)) as executor:
            future_to_task = {}
            for subtask in subtasks:
                worker = self.worker_factory()
                future = executor.submit(worker.execute, subtask)
                future_to_task[future] = subtask

                self._emit_progress("supplementary_worker_started", {
                    "task_id": subtask.id,
                    "task_name": subtask.name,
                })

            for future in as_completed(future_to_task):
                subtask = future_to_task[future]
                try:
                    result = future.result(timeout=AGENT_CONFIG["timeout_per_worker"])
                    results.append(result)

                    self._emit_progress("supplementary_worker_completed", {
                        "task_id": subtask.id,
                        "task_name": subtask.name,
                        "success": True,
                    })
                except Exception as e:
                    results.append({
                        "task_id": subtask.id,
                        "task_name": subtask.name,
                        "error": str(e),
                        "success": False,
                    })

                    self._emit_progress("supplementary_worker_failed", {
                        "task_id": subtask.id,
                        "task_name": subtask.name,
                        "error": str(e),
                    })

        return results

    def refine_report(
        self,
        original_task: str,
        original_report: str,
        gap_analysis: dict,
        supplementary_results: list[dict],
        refinement_focus: list[str],
        verification_tasks: list[str] = None,
    ) -> str:
        """
        åŸºäºè¡¥å……ç ”ç©¶ç»“æœä¿®è®¢æŠ¥å‘Šï¼ˆå¢å¼ºç‰ˆï¼‰

        å‚æ•°:
            original_task: åŸå§‹ç ”ç©¶ä»»åŠ¡
            original_report: åŸå§‹æŠ¥å‘Š
            gap_analysis: å·®è·åˆ†æç»“æœ
            supplementary_results: è¡¥å……ç ”ç©¶ç»“æœ
            refinement_focus: ä¿®è®¢é‡ç‚¹
            verification_tasks: éœ€è¦éªŒè¯çš„äº‹å®

        è¿”å›:
            str: ä¿®è®¢åçš„æŠ¥å‘Š
        """
        self._emit_progress("refining_report", {})

        # æ ¼å¼åŒ–è¡¥å……ç ”ç©¶ç»“æœ
        supplementary_text = self._format_worker_results(supplementary_results)

        prompt = REPORT_REFINEMENT_PROMPT.format(
            original_task=original_task,
            original_report=original_report,
            gap_analysis=json.dumps(gap_analysis, ensure_ascii=False, indent=2),
            supplementary_results=supplementary_text,
            refinement_focus="\n".join(f"- {f}" for f in refinement_focus),
            verification_tasks="\n".join(f"- {t}" for t in (verification_tasks or [])) or "æ— ",
        )

        response = self.llm.chat(
            prompt=prompt,
            system_prompt=ORCHESTRATOR_SYSTEM_PROMPT,
            temperature=0.5,
            max_tokens=8192,
        )

        if not response.success:
            # å¦‚æœä¿®è®¢å¤±è´¥ï¼Œè¿”å›åŸæŠ¥å‘Š
            return original_report

        self._emit_progress("report_refined", {
            "new_length": len(response.content),
        })

        return response.content

    def run(self, task: str) -> ResearchState:
        """
        æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶æµç¨‹ï¼ˆå¢å¼ºç‰ˆ - å¸¦æ™ºèƒ½ç»ˆæ­¢ï¼‰

        å‚æ•°:
            task: ç ”ç©¶ä»»åŠ¡æè¿°

        è¿”å›:
            ResearchState: ç ”ç©¶çŠ¶æ€ï¼ˆåŒ…å«æœ€ç»ˆæŠ¥å‘Šï¼‰

        è¿­ä»£ç­–ç•¥:
            - æœ€å°‘è¿­ä»£ min_iterations æ¬¡ï¼ˆé»˜è®¤2æ¬¡ï¼‰
            - æœ€å¤šè¿­ä»£ max_iterations æ¬¡ï¼ˆé»˜è®¤5æ¬¡ï¼‰
            - è¾¾åˆ°è´¨é‡é˜ˆå€¼åï¼Œè‹¥å·²æ»¡è¶³æœ€å°‘è¿­ä»£æ¬¡æ•°åˆ™åœæ­¢
            - æ£€æµ‹åˆ°æ”¶ç›Šé€’å‡æ—¶æå‰ç»ˆæ­¢
        """
        # è·å–è¿­ä»£é…ç½®
        min_iterations = AGENT_CONFIG.get("min_iterations", 2)
        max_iterations = AGENT_CONFIG.get("max_iterations", 5)
        quality_threshold = AGENT_CONFIG.get("quality_threshold", 80)
        enable_diminishing_returns_check = AGENT_CONFIG.get("enable_diminishing_returns_check", True)

        state = ResearchState(
            original_task=task,
            start_time=time.time(),
            status="initialized",
        )

        try:
            # ========== é˜¶æ®µ1: ä»»åŠ¡æ‹†è§£ï¼ˆå«æŸ¥è¯¢ç±»å‹åˆ¤æ–­ï¼‰ ==========
            state.status = "planning"
            state.task_plan = self.decompose_task(task)

            # è®°å½•æŸ¥è¯¢ç±»å‹
            if state.task_plan.query_type:
                state.query_type = state.task_plan.query_type.query_type

            # ========== é˜¶æ®µ2: åˆå§‹ç ”ç©¶ ==========
            state.status = "researching"
            state.worker_results = self.dispatch_workers(state.task_plan)

            # ========== é˜¶æ®µ3: ç”Ÿæˆåˆå§‹æŠ¥å‘Š ==========
            state.status = "synthesizing"
            current_report = self.synthesize_results(
                task,
                state.task_plan,
                state.worker_results,
            )

            # ========== é˜¶æ®µ4: è¿­ä»£ä¼˜åŒ–å¾ªç¯ï¼ˆå¸¦æ™ºèƒ½ç»ˆæ­¢ï¼‰ ==========
            state.status = "iterating"
            previous_score = 0

            for iteration in range(1, max_iterations + 1):
                self._emit_progress("iteration_start", {
                    "iteration": iteration,
                    "max_iterations": max_iterations,
                    "min_iterations": min_iterations,
                })

                # 4.1 è´¨é‡æ£€æŸ¥
                quality_result = self.check_quality(
                    current_report,
                    task,
                    state.query_type,
                )
                current_score = quality_result.get("overall_score", 0)
                score_improvement = current_score - previous_score

                # è®°å½•æœ¬æ¬¡è¿­ä»£
                iteration_record = IterationRecord(
                    iteration=iteration,
                    report=current_report,
                    quality_score=current_score,
                    quality_result=quality_result,
                    score_improvement=score_improvement,
                )
                state.iteration_history.append(iteration_record)
                state.iteration_count = iteration

                self._emit_progress("iteration_evaluated", {
                    "iteration": iteration,
                    "score": current_score,
                    "improvement": score_improvement,
                    "threshold": quality_threshold,
                    "passed": current_score >= quality_threshold,
                })

                # 4.2 åˆ¤æ–­æ˜¯å¦å¯ä»¥ç»“æŸè¿­ä»£
                # æ¡ä»¶ï¼šå·²è¾¾åˆ°æœ€å°‘è¿­ä»£æ¬¡æ•° ä¸” è´¨é‡é€šè¿‡é˜ˆå€¼
                if iteration >= min_iterations and current_score >= quality_threshold:
                    self._emit_progress("quality_passed", {
                        "iteration": iteration,
                        "score": current_score,
                        "message": f"è´¨é‡æ£€æŸ¥é€šè¿‡ï¼ˆ{current_score} >= {quality_threshold}ï¼‰ï¼Œè¿­ä»£ç»“æŸ",
                    })
                    break

                # 4.3 æ£€æŸ¥æ”¶ç›Šé€’å‡ï¼ˆå¯é€‰ï¼‰
                if enable_diminishing_returns_check and iteration >= 2:
                    dr_result = self.check_diminishing_returns(
                        original_task=task,
                        current_score=current_score,
                        iteration_history=state.iteration_history,
                        recent_supplementary_results=iteration_record.supplementary_results,
                    )

                    if dr_result.get("diminishing_returns_detected", False):
                        iteration_record.diminishing_returns_detected = True
                        recommendation = dr_result.get("recommendation", {})

                        if recommendation.get("action") == "stop":
                            state.early_termination_reason = "æ”¶ç›Šé€’å‡æ£€æµ‹è§¦å‘æå‰ç»ˆæ­¢"
                            self._emit_progress("diminishing_returns_stop", {
                                "iteration": iteration,
                                "score": current_score,
                                "reasoning": recommendation.get("reasoning", ""),
                            })
                            break

                # 4.4 å¦‚æœè¿˜æ²¡åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œç»§ç»­ä¼˜åŒ–
                if iteration < max_iterations:
                    self._emit_progress("iteration_continue", {
                        "iteration": iteration,
                        "reason": "æœªè¾¾è´¨é‡é˜ˆå€¼" if current_score < quality_threshold else "æœªè¾¾æœ€å°‘è¿­ä»£æ¬¡æ•°",
                    })

                    # 4.4.1 å·®è·åˆ†æ
                    gap_result = self.analyze_gaps(
                        original_task=task,
                        current_report=current_report,
                        quality_result=quality_result,
                        iteration=iteration,
                        max_iterations=max_iterations,
                        query_type=state.query_type,
                    )

                    iteration_record.gap_analysis = gap_result.get("gap_analysis", {})

                    # æ£€æŸ¥æ˜¯å¦å»ºè®®åœæ­¢è¿­ä»£
                    stop_recommendation = gap_result.get("stop_iteration_recommendation", {})
                    if stop_recommendation.get("should_stop", False):
                        state.early_termination_reason = stop_recommendation.get("reasoning", "å·®è·åˆ†æå»ºè®®åœæ­¢")
                        self._emit_progress("gap_analysis_stop", {
                            "iteration": iteration,
                            "reasoning": stop_recommendation.get("reasoning", ""),
                        })
                        break

                    # 4.4.2 æ‰§è¡Œè¡¥å……ç ”ç©¶
                    supplementary_tasks = gap_result.get("supplementary_tasks", [])
                    if supplementary_tasks:
                        supplementary_results = self.dispatch_supplementary_workers(supplementary_tasks)
                        iteration_record.supplementary_results = supplementary_results

                        # 4.4.3 ä¿®è®¢æŠ¥å‘Š
                        current_report = self.refine_report(
                            original_task=task,
                            original_report=current_report,
                            gap_analysis=gap_result.get("gap_analysis", {}),
                            supplementary_results=supplementary_results,
                            refinement_focus=gap_result.get("refinement_focus", []),
                            verification_tasks=gap_result.get("verification_tasks", []),
                        )
                    else:
                        # æ²¡æœ‰è¡¥å……ä»»åŠ¡ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€æ¬¡è¯„ä¼°
                        self._emit_progress("no_supplementary_tasks", {
                            "iteration": iteration,
                        })

                previous_score = current_score

                self._emit_progress("iteration_end", {
                    "iteration": iteration,
                })

            # ========== é˜¶æ®µ5: å®Œæˆ ==========
            state.final_report = current_report
            state.quality_score = state.iteration_history[-1].quality_score if state.iteration_history else 0
            state.status = "completed"
            state.end_time = time.time()

            self._emit_progress("completed", {
                "duration": state.end_time - state.start_time,
                "quality_score": state.quality_score,
                "iterations": state.iteration_count,
                "query_type": state.query_type,
                "early_termination": bool(state.early_termination_reason),
                "early_termination_reason": state.early_termination_reason,
            })

        except Exception as e:
            state.status = "failed"
            state.error_msg = str(e)
            state.end_time = time.time()

            self._emit_progress("failed", {
                "error": str(e),
            })

        return state

    def _format_worker_results(self, results: list[dict]) -> str:
        """æ ¼å¼åŒ– Worker ç»“æœä¸ºæ–‡æœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        parts = []
        for i, result in enumerate(results, 1):
            if result.get("success", True) and "error" not in result:
                # æå– OODA å¾ªç¯ä¿¡æ¯
                ooda_summary = ""
                ooda_cycles = result.get("ooda_cycles", [])
                if ooda_cycles:
                    ooda_summary = f"\nã€OODAå¾ªç¯ã€‘æ‰§è¡Œäº† {len(ooda_cycles)} è½®åˆ†æ"

                # æå–æ¥æºè´¨é‡ä¿¡æ¯
                source_quality = result.get("source_quality_assessment", {})
                source_summary = ""
                if source_quality:
                    high_quality = source_quality.get("high_quality_sources", [])
                    questionable = source_quality.get("questionable_sources", [])
                    if high_quality or questionable:
                        source_summary = f"\nã€æ¥æºè´¨é‡ã€‘ä¼˜è´¨æ¥æº: {len(high_quality)}ä¸ª, å¯ç–‘æ¥æº: {len(questionable)}ä¸ª"

                # æå–äº‹å®ä¸æ¨æµ‹åŒºåˆ†
                speculative_info = result.get("speculative_vs_factual", {})
                spec_summary = ""
                if speculative_info:
                    verified = speculative_info.get("verified_facts", [])
                    speculative = speculative_info.get("speculative_claims", [])
                    if verified or speculative:
                        spec_summary = f"\nã€äº‹å®éªŒè¯ã€‘å·²éªŒè¯: {len(verified)}é¡¹, æ¨æµ‹æ€§: {len(speculative)}é¡¹"

                parts.append(f"""
--- Worker {i} ç ”ç©¶æˆæœ ---
ä»»åŠ¡: {result.get('task_name', 'æœªçŸ¥')}
{ooda_summary}

ã€å…³é”®å‘ç°ã€‘
{self._format_findings(result.get('key_findings', []))}

ã€ç ”ç©¶æ€»ç»“ã€‘
{result.get('summary', 'æ— ')}

ã€æ•°æ®ç‚¹ã€‘
{self._format_data_points(result.get('data_points', []))}

ã€æ´å¯Ÿã€‘
{chr(10).join('â€¢ ' + insight for insight in result.get('insights', []))}
{source_summary}
{spec_summary}

ã€ç½®ä¿¡åº¦ã€‘{result.get('confidence_level', 'æœªçŸ¥')}
ã€ç½®ä¿¡åº¦ä¾æ®ã€‘{result.get('confidence_reasoning', 'æ— ')}

ã€ç ”ç©¶å±€é™æ€§ã€‘{result.get('limitations', 'æ— ')}
ã€ä¿¡æ¯ç©ºç™½ã€‘{', '.join(result.get('information_gaps', [])) or 'æ— '}
ã€ç»ˆæ­¢åŸå› ã€‘{result.get('termination_reason', 'æœªçŸ¥')}
""")
            else:
                parts.append(f"""
--- Worker {i} ---
ä»»åŠ¡: {result.get('task_name', 'æœªçŸ¥')}
çŠ¶æ€: å¤±è´¥
é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}
""")

        return "\n".join(parts)

    def _format_findings(self, findings: list[dict]) -> str:
        """æ ¼å¼åŒ–å…³é”®å‘ç°ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not findings:
            return "æ— "
        lines = []
        for f in findings:
            lines.append(f"â€¢ {f.get('finding', '')}")
            if f.get('data'):
                lines.append(f"  æ•°æ®: {f.get('data')}")
            if f.get('source'):
                reliability = f.get('reliability', 'æœªçŸ¥')
                source_type = f.get('source_type', '')
                verified = "âœ“" if f.get('is_verified') else "?"
                lines.append(f"  æ¥æº: {f.get('source')} [{reliability}å¯é åº¦] [{source_type}] {verified}")
            if f.get('reliability_reasoning'):
                lines.append(f"  å¯é æ€§ä¾æ®: {f.get('reliability_reasoning')}")
        return "\n".join(lines)

    def _format_data_points(self, data_points: list[dict]) -> str:
        """æ ¼å¼åŒ–æ•°æ®ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not data_points:
            return "æ— "
        lines = []
        for dp in data_points:
            confidence = dp.get('confidence', 'æœªçŸ¥')
            confidence_icon = "ğŸŸ¢" if confidence == "high" else ("ğŸŸ¡" if confidence == "medium" else "ğŸ”´")
            lines.append(f"â€¢ {dp.get('metric', '')}: {dp.get('value', '')} (æ¥æº: {dp.get('source', 'æœªçŸ¥')}) {confidence_icon}")
        return "\n".join(lines)
